{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning OpenAI API GPT Models for Sentiment Analysis With Weights & Biases\n",
    "\n",
    "### https://wandb.ai/mostafaibrahim17/ml-articles/reports/Fine-Tuning-ChatGPT-for-Sentiment-Analysis-With-W-B--Vmlldzo1NjMzMjQx \n",
    "\n",
    "This notebook explores fine-tuning GPT Models for sentiment analysis using Weights & Biases. Our experiment will lead to an overall accuracy boost, and we'll delve into applications. In today's data-driven world, sentiment analysis plays a pivotal role in discerning public opinion on a myriad of topics. Advanced models like GPT Model, built on the GPT architecture, offer immense potential in understanding and interpreting human emotions from textual data. However, like many tools, their out-of-the-box capabilities might not capture the nuanced intricacies of sentiment, especially in diverse datasets like those from Reddit. This article dives deep into the process of fine-tuning GPT Models for sentiment analysis, utilizing the powerful features of the Weights & Biases platform, and delves into the improvements and challenges faced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- How Can a GPT Model Be Used for Sentiment Analysis?\n",
    "- Fine-Tuning GPT Models for Sentiment Analysis\n",
    "- Data Preparation and Labeling\n",
    "  - The Current Data Set at Hand\n",
    "  - Data Augmentation Sentiment Analysis Dataset for Fine-Tuning\n",
    "  - The Importance of High-Quality Training Data for Sentiment Analysis\n",
    "- Step-by-Step Tutorial\n",
    "  - Evaluating the Old Model’s Performance\n",
    "  - Fine-Tuning the GPT Model\n",
    "  - Evaluating the New Model’s Performance\n",
    "- Fine-Tuning Results and Analysis\n",
    "- Practical Applications and Use Cases\n",
    "  - Jargon and Slang Understanding\n",
    "  - E-Commerce Product Reviews\n",
    "- Further Improvements\n",
    "- Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Can a GPT Model Be Used for Sentiment Analysis?\n",
    "\n",
    "GPT Model's ability to understand natural language makes it a good fit for sentiment analysis. This is because, unlike traditional chatbots that rely on predefined responses, GPT Models generate real-time answers based on a vast amount of training data. This approach enables it to provide responses that are contextually relevant and informed by a broad spectrum of information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT Model for Sentiment Analysis\n",
    "\n",
    "Fine-tuning is a pivotal step in adapting a general-purpose models, like GPT Models, to a specific task such as sentiment analysis. A GPT Model, with its broad language understanding capabilities, can grasp a vast array of topics and concepts. However, sentiment analysis is more than just comprehending text; it requires a nuanced understanding of subjective tones, moods, and emotions.\n",
    "<br/><br/>\n",
    "Think about sarcasm. Understanding sarcasm is tricky, even for humans sometimes. Sarcasm is when we say something but mean the opposite, often in a joking or mocking way. For example, if it starts raining just as you're about to go outside, and you say, \"Oh, perfect timing!\" you're probably being sarcastic because it's actually bad timing. Now, imagine a machine trying to understand this. Without special training, it might think you're genuinely happy about the rain because you said \"perfect.\" This is where fine-tuning a model like GPT Model becomes crucial.\n",
    "<br/><br/>\n",
    "GPT Model, out of the box, is pretty good at understanding a lot of text. It's read more than most humans ever will. But sarcasm is subtle and often needs context. So, to make GPT Models really get sarcasm, we'd expose it to many examples of sarcastic sentences until it starts catching on to the patterns. But here's the catch: sarcasm doesn't look the same everywhere. In different cultures or situations, what's sarcastic in one place might be meant seriously in another. That's why just general knowledge isn't enough. The model needs specific examples to truly grasp the playful twists and turns of sarcasm. In short, to make GPT Model understand sarcasm like a human, it needs extra training on it, just like someone might need to watch several comedy shows to start understanding a comedian's sense of humor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Labeling\n",
    "\n",
    "### The Current Data Set at Hand\n",
    "\n",
    "In this notebook, we'll leverage a Reddit dataset sourced from Kaggle, available here: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset. This dataset features two key columns: clean_comment(the sentiment text) and its corresponding category (sentiment label).\n",
    "<br/><br/>\n",
    "The File Contains 37k comments along with its Sentimental Labelling. All the Comments in the dataset are cleaned and assigned with a Sentiment Label. These Comments Dataset Can Be used to Build a Sentimental Analysis Machine Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Sentiment Analysis Dataset for Fine-Tuning\n",
    "\n",
    "It's important to note that the refined Fine-Tuning GPT Model process mandates a specific data structure in a JSONL file for optimal training. \n",
    "\n",
    "#### What is a JSONL File?\n",
    "\n",
    "A `.jsonl` file (short for JSON Lines) is a file format used to store structured data, typically for machine learning and data processing applications. Each line in a `.jsonl` file is a separate, self-contained JSON object. This makes it particularly useful for handling large datasets that can be processed line-by-line, avoiding loading everything into memory at once.\n",
    "\n",
    "##### Key Features of JSONL Format:\n",
    "- **One JSON Object Per Line:** Each line in the file is an independent JSON object.\n",
    "- **Line-Delimited:** The objects are separated by newlines (`\\n`), not by commas or brackets as in standard JSON.\n",
    "- **Efficient Parsing:** Line-by-line processing is easy and efficient, which is helpful when working with large datasets.\n",
    "- **No Root Structure:** Unlike regular JSON, there is no outer array or object enclosing the entire dataset.\n",
    "\n",
    "##### Example of a JSONL File:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "\n",
    "```\n",
    "##### Common Use Cases:\n",
    "- **Training Data for Machine Learning Models:** Frequently used in NLP tasks where each line contains an individual record (e.g., a sentence with a label).\n",
    "- **Log Data Storage:** Each log entry is a separate JSON object.\n",
    "- **Streaming Data Processing:** Ideal for scenarios where you process data incrementally.\n",
    "\n",
    "##### How to Work with JSONL:\n",
    "- **Reading and Writing:** In Python, you can use the `json` or `jsonlines` library to read and write JSONL files.\n",
    "- **Tools:** Many tools like `jq`, Pandas, and other data processing libraries support the JSONL format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of High-Quality Training Data for Sentiment Analysis\n",
    "\n",
    "High-quality training data is pivotal for sentiment analysis as it ensures the model learns to accurately distinguish nuances in emotions. Poor data can lead to misinterpretations, reducing the effectiveness of the analysis. Moreover, comprehensive and well-curated data can significantly boost the model's ability to generalize across diverse real-world scenarios. The dataset we're utilizing underscores this point. As even some of its entries are so nuanced that even humans might struggle to discern their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial\n",
    "\n",
    "### Evaluating the Old Model's Performance\n",
    "\n",
    "#### Step 1: Installing and Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't already installed the required libraries, you can do so by running the following commands:\n",
    "# !pip install openai\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import os\n",
    "import openai\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path  # Handles filesystem paths in an object-oriented way\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Creating our client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our client with the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Loading and Processing the Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       clean_comment  category\n",
      "0   family mormon have never tried explain them t...         1\n",
      "1  buddhism has very much lot compatible with chr...         1\n",
      "2  seriously don say thing first all they won get...        -1\n",
      "3  what you have learned yours and only yours wha...         0\n",
      "4  for your own benefit you may want read living ...         1\n",
      "5  you should all sit down together and watch the...        -1\n",
      "6   was teens when discovered zen meditation was ...         1\n",
      "7                           jesus was zen meets jew          0\n",
      "8  there are two varieties christians dogmatic th...        -1\n",
      "9  dont worry about trying explain yourself just ...         1\n"
     ]
    }
   ],
   "source": [
    "# Put the file path in a variable\n",
    "filename = \"./practical_data/reddit_data.csv\"\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Drop rows with NaN values in 'clean_comment' and 'category'\n",
    "df.dropna(subset=['clean_comment', 'category'], inplace=True)\n",
    "\n",
    "\n",
    "# Show the first 10 rows\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Initializing a New Weights & Biases Project\n",
    "\n",
    "Now for something new. We will create a new WandB project and turn on autologging to track our progress. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msuspicious-cow\u001b[0m (\u001b[33msuspicious-cow-self\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Zain_\\Dropbox\\Personal\\Data Science Projects\\OpenAI_API_Fine_Tuning\\wandb\\run-20240817_125729-kse879k8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis/runs/kse879k8' target=\"_blank\">exalted-voice-14</a></strong> to <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis/runs/kse879k8' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis/runs/kse879k8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis/runs/kse879k8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x26904e19100>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the environment variable for the notebook name\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"fine_tune_openai_sentiment_wandb.ipynb\"\n",
    "\n",
    "# Create a new Weights and Biases project\n",
    "wandb.init(project=\"Reddit_Sentiment_Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Take a Sample to Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a sample of 100 rows\n",
    "df_sample = df.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Define Helper Functions to Convert Model Response to Sentiment Value and Vice Versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_response_to_sentiment(response):\n",
    "    response = response.lower()\n",
    "    if 'positive' in response:\n",
    "        return 1\n",
    "    elif 'negative' in response:\n",
    "        return -1\n",
    "    elif 'neutral' in response:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # Unknown sentiment\n",
    "    \n",
    "def convert_numeric_to_string_sentiment(value):\n",
    "    if value == 1:\n",
    "        return \"positive\"\n",
    "    elif value == -1:\n",
    "        return \"negative\"\n",
    "    elif value == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluating the Current Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/100 rows.\n",
      "Processed 2/100 rows.\n",
      "Processed 3/100 rows.\n",
      "Processed 4/100 rows.\n",
      "Processed 5/100 rows.\n",
      "Processed 6/100 rows.\n",
      "Processed 7/100 rows.\n",
      "Processed 8/100 rows.\n",
      "Processed 9/100 rows.\n",
      "Processed 10/100 rows.\n",
      "Processed 11/100 rows.\n",
      "Processed 12/100 rows.\n",
      "Processed 13/100 rows.\n",
      "Processed 14/100 rows.\n",
      "Processed 15/100 rows.\n",
      "Processed 16/100 rows.\n",
      "Processed 17/100 rows.\n",
      "Processed 18/100 rows.\n",
      "Processed 19/100 rows.\n",
      "Processed 20/100 rows.\n",
      "Processed 21/100 rows.\n",
      "Processed 22/100 rows.\n",
      "Processed 23/100 rows.\n",
      "Processed 24/100 rows.\n",
      "Processed 25/100 rows.\n",
      "Processed 26/100 rows.\n",
      "Processed 27/100 rows.\n",
      "Processed 28/100 rows.\n",
      "Processed 29/100 rows.\n",
      "Processed 30/100 rows.\n",
      "Processed 31/100 rows.\n",
      "Processed 32/100 rows.\n",
      "Processed 33/100 rows.\n",
      "Processed 34/100 rows.\n",
      "Processed 35/100 rows.\n",
      "Processed 36/100 rows.\n",
      "Processed 37/100 rows.\n",
      "Processed 38/100 rows.\n",
      "Processed 39/100 rows.\n",
      "Processed 40/100 rows.\n",
      "Processed 41/100 rows.\n",
      "Processed 42/100 rows.\n",
      "Processed 43/100 rows.\n",
      "Processed 44/100 rows.\n",
      "Processed 45/100 rows.\n",
      "Processed 46/100 rows.\n",
      "Processed 47/100 rows.\n",
      "Processed 48/100 rows.\n",
      "Processed 49/100 rows.\n",
      "Processed 50/100 rows.\n",
      "Processed 51/100 rows.\n",
      "Processed 52/100 rows.\n",
      "Processed 53/100 rows.\n",
      "Processed 54/100 rows.\n",
      "Processed 55/100 rows.\n",
      "Processed 56/100 rows.\n",
      "Processed 57/100 rows.\n",
      "Processed 58/100 rows.\n",
      "Processed 59/100 rows.\n",
      "Processed 60/100 rows.\n",
      "Processed 61/100 rows.\n",
      "Processed 62/100 rows.\n",
      "Processed 63/100 rows.\n",
      "Processed 64/100 rows.\n",
      "Processed 65/100 rows.\n",
      "Processed 66/100 rows.\n",
      "Processed 67/100 rows.\n",
      "Processed 68/100 rows.\n",
      "Processed 69/100 rows.\n",
      "Processed 70/100 rows.\n",
      "Processed 71/100 rows.\n",
      "Processed 72/100 rows.\n",
      "Processed 73/100 rows.\n",
      "Processed 74/100 rows.\n",
      "Processed 75/100 rows.\n",
      "Processed 76/100 rows.\n",
      "Processed 77/100 rows.\n",
      "Processed 78/100 rows.\n",
      "Processed 79/100 rows.\n",
      "Processed 80/100 rows.\n",
      "Processed 81/100 rows.\n",
      "Processed 82/100 rows.\n",
      "Processed 83/100 rows.\n",
      "Processed 84/100 rows.\n",
      "Processed 85/100 rows.\n",
      "Processed 86/100 rows.\n",
      "Processed 87/100 rows.\n",
      "Processed 88/100 rows.\n",
      "Processed 89/100 rows.\n",
      "Processed 90/100 rows.\n",
      "Processed 91/100 rows.\n",
      "Processed 92/100 rows.\n",
      "Processed 93/100 rows.\n",
      "Processed 94/100 rows.\n",
      "Processed 95/100 rows.\n",
      "Processed 96/100 rows.\n",
      "Processed 97/100 rows.\n",
      "Processed 98/100 rows.\n",
      "Processed 99/100 rows.\n",
      "Processed 100/100 rows.\n"
     ]
    }
   ],
   "source": [
    "client = openai.Client()\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "loop_count = 0  # Counter for loop iterations\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_sample.iterrows():\n",
    "    loop_count += 1  # Increment the loop count\n",
    "    text = row['clean_comment']  # Adjusted column name\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"What is the sentiment of the following text? Please respond with 'positive', 'negative', or 'neutral'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        predicted_sentiment = convert_response_to_sentiment(response)\n",
    "        \n",
    "        results.append({\n",
    "        \"sentiment\": text,  \n",
    "        \"labeled_prediction\": convert_numeric_to_string_sentiment(row['category']),\n",
    "        \"old_model_prediction\": response\n",
    "    })\n",
    "        \n",
    "        # Check if the predicted sentiment matches the actual sentiment\n",
    "        if predicted_sentiment == row['category']:  # Adjusted column name\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        # Print the current progress using loop_count\n",
    "        total_rows = len(df_sample)\n",
    "        print(f\"Processed {loop_count}/{total_rows} rows.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {index}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Calculating the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_predictions / total_rows) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Logging the Accuracy to WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy before: 45.00%\n"
     ]
    }
   ],
   "source": [
    "wandb.log({\"Old Accuracy\": accuracy})\n",
    "print(f'Model Accuracy before: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Model\n",
    "\n",
    "#### Step 10: Converting the Dataframe to JSONL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"reddit_sentiment_data.jsonl\"\n",
    "\n",
    "\n",
    "# Convert DataFrame to the desired JSONL format\n",
    "with open(output_filename, \"w\") as file:\n",
    "    for _, row in df.iterrows():\n",
    "        # Map the target to its corresponding string label\n",
    "        target_label = {\n",
    "            0: 'neutral',\n",
    "            1: 'positive',  # Corrected the spelling here\n",
    "            -1: 'negative'\n",
    "        }.get(row['category'], 'unknown')\n",
    "        \n",
    "        data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"What is the sentiment of the following text? Please respond with 'positive', 'negative', or 'neutral'.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": row['clean_comment']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": target_label\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write each data point as a separate line in the JSONL file\n",
    "        file.write(json.dumps(data) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10b: Create the Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split Function for JSONL Files\n",
    "def split_jsonl_file(file_path, train_ratio=0.8):\n",
    "    # Read the input file\n",
    "    file_path = Path(file_path)\n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    # Prepare output file paths\n",
    "    train_file = file_path.with_name(f\"{file_path.stem}_train{file_path.suffix}\")\n",
    "    test_file = file_path.with_name(f\"{file_path.stem}_test{file_path.suffix}\")\n",
    "    \n",
    "    # Write train data\n",
    "    with train_file.open('w', encoding='utf-8') as f:\n",
    "        for item in train_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # Write test data\n",
    "    with test_file.open('w', encoding='utf-8') as f:\n",
    "        for item in test_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Train data saved to: {train_file}\")\n",
    "    print(f\"Test data saved to: {test_file}\")\n",
    "    print(f\"Train set size: {len(train_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    \n",
    "    return(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: reddit_sentiment_data_train.jsonl\n",
      "Test data saved to: reddit_sentiment_data_test.jsonl\n",
      "Train set size: 29719\n",
      "Test set size: 7430\n",
      "\n",
      "\n",
      "Train file path: reddit_sentiment_data_train.jsonl\n",
      "Test file path: reddit_sentiment_data_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Make our train/test files\n",
    "# File paths and data processing\n",
    "file_path = output_filename\n",
    "\n",
    "# Split the JSONL file into train and test sets\n",
    "train_test_files = split_jsonl_file(file_path)\n",
    "print(\"\\n\")  # Print a blank line for better output readability\n",
    "\n",
    "# Convert the returned file paths to strings\n",
    "train_path, test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "# Print the paths of the resulting train and test files\n",
    "print(f\"Train file path: {train_path}\")\n",
    "print(f\"Test file path: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11: Upload the files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "train__set_file = client.files.create(\n",
    "            file=open(train_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )\n",
    "\n",
    "# Upload the test data to the OpenAI API\n",
    "test_set_file = client.files.create(\n",
    "            file=open(test_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12: Create a Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "wandb_params_ft_job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Base model to be fine-tuned\n",
    "    training_file=train__set_file.id,  # ID of the uploaded training data file\n",
    "    validation_file=test_set_file.id,  # ID of the uploaded validation (test) data file\n",
    "    hyperparameters={\n",
    "        \"batch_size\": \"auto\",  # Let API automatically determine batch size\n",
    "        \"learning_rate_multiplier\": \"auto\",  # Auto-set learning rate multiplier\n",
    "        \"n_epochs\": \"auto\",  # Automatically decide number of training epochs\n",
    "    },\n",
    "    suffix=\"reddit_sentiment\",  # Append this to the fine-tuned model's name\n",
    "    integrations=[\n",
    "        {\n",
    "            \"type\": \"wandb\",\n",
    "            \"wandb\": {\n",
    "                \"project\": \"Reddit_Sentiment_Analysis\",  # Replace with your actual project name\n",
    "                \"name\": \"Reddit_Sentiment_Analysis_Run_001\",  # Optional: Replace with your desired run name or remove\n",
    "                \"entity\": \"suspicious-cow-self\",  # Optional: Replace with your entity or remove\n",
    "                \"tags\": [\"reddit\", \"sentiment\"]  # Optional: Replace with your desired tags or remove\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    seed=None,  # Specific random seed set for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the New Model's Performance\n",
    "\n",
    "#### Step 13a: Looking at the metrics from WandB\n",
    "\n",
    "We will do manual calculations later for fun but, for now, let's look at the data from WandB. There are two ways you can do this:\n",
    "1. Through the WandB website\n",
    "2. Through code\n",
    "<br/><br/>\n",
    "We will do both of these. \n",
    "\n",
    "First, for the website go to https://wandb.ai/suspicious-cow-self/projects and click on the Reddit_Sentiment_Analysis project. This should automatically show you the results from the latest run in a graphical format. \n",
    "\n",
    "Second, let's manually compute rough statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
